{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04_clrp_training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LZp4MeZUdFsm"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVxUi7YlWDq6"
      },
      "source": [
        "# README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwTVLCTXWGuR"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M6JW9m3nWIf-"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install sentence-transformers\n",
        "!pip install sklearn\n",
        "!pip install datasets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VgAHilC2WZ16"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import itertools\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import gzip\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sentence_transformers import SentenceTransformer, util, losses, models\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForMaskedLM, DataCollatorForWholeWordMask, DataCollatorForLanguageModeling, pipeline\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, TrainerCallback\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import shutil\n",
        "from datasets import load_metric\n",
        "import gc\n",
        "gc.enable()\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import Lasso, BayesianRidge, Perceptron, SGDRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FqQyqJrWYblS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtQiZchuYObs"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPuHJc1eYQJE"
      },
      "source": [
        "BASE_PATH = 'gdrive/My Drive/colabNotebooks/commonLitReadabilityPrize/firstPlace_CodeFiles'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIOcjB6-bHPj"
      },
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "SEED = 28\n",
        "seed_everything(seed=SEED)\n",
        "MAX_LENGTH = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEj-F1o6xHZC"
      },
      "source": [
        "\n",
        "# fine-tuned model paths\n",
        "# adjust path if you have saved the models in different directories\n",
        "ALBERT_TRAINED_1 = os.path.join(BASE_PATH, 'models/albertxxlarge2models')\n",
        "ALBERT_TRAINED_2 = os.path.join(BASE_PATH, 'models/albertxxlargelowlr')\n",
        "ALBERT_TRAINED_3 = os.path.join(BASE_PATH, 'models/albertxxlargealldata')\n",
        "DEBERTA_TRAINED_1 = os.path.join(BASE_PATH, 'models/debertalarge')\n",
        "DEBERTA_TRAINED_2 = os.path.join(BASE_PATH, 'models/debertalargelowlr')\n",
        "DEBERTA_TRAINED_3 = os.path.join(BASE_PATH, 'models/debertabootstrap')\n",
        "ROBERTA_TRAINED_1 = os.path.join(BASE_PATH, 'models/robertalargetwomodels')\n",
        "ELECTRA_TRAINED_1 = os.path.join(BASE_PATH, 'models/electralarge')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkLqmtm8Wsck"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3o18TZGWtqM"
      },
      "source": [
        "def train_model(\n",
        "    model_dir,\n",
        "    out_dir,\n",
        "    data,\n",
        "    data_labels,\n",
        "    test_data=None,\n",
        "    test_labels=None,\n",
        "    do_eval=False,\n",
        "    do_epoch_eval=False,\n",
        "    do_save_best=False,\n",
        "    hyperparams={'bs': 16, 'lr': 1e-4, 'ep': 5, 'bias': False, 'init': None},\n",
        "    cfg={'num_labels': 1, 'logging_steps': 500, 'is_multilabel': False, 'keep_layers': None}\n",
        "    ):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "  \n",
        "  train_encodings = tokenizer(data, truncation=True, padding=True, max_length=MAX_LENGTH)\n",
        "  if test_data:\n",
        "    test_encodings = tokenizer(test_data, truncation=True, padding=True, max_length=MAX_LENGTH)\n",
        "  \n",
        "\n",
        "  class LitDataset(torch.utils.data.Dataset):\n",
        "      def __init__(self, encodings, labels):\n",
        "          self.encodings = encodings\n",
        "          self.labels = labels\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "          item['labels'] = torch.tensor(self.labels[idx])\n",
        "          return item\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.labels)\n",
        "\n",
        "  train_dataset = LitDataset(train_encodings, data_labels)\n",
        "  if test_data:\n",
        "    test_dataset = LitDataset(test_encodings, test_labels)\n",
        "  \n",
        "  train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=hyperparams['bs'])\n",
        "  training_steps = len(train_dataloader) * hyperparams['ep'] \n",
        "  warmup_steps = math.ceil(training_steps  * 0.06)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=out_dir,          # output directory\n",
        "      num_train_epochs=hyperparams['ep'],              # total number of training epochs\n",
        "      per_device_train_batch_size=hyperparams['bs'],  # batch size per device during training\n",
        "      per_device_eval_batch_size=1,   # batch size for evaluationing rate scheduler\n",
        "      logging_dir='/tmp/logs',            # directory for storing logs\n",
        "      logging_steps=cfg['logging_steps'],\n",
        "      seed=SEED,\n",
        "      weight_decay=hyperparams['weight_decay'],\n",
        "      learning_rate=hyperparams['lr'],\n",
        "      save_strategy='no'\n",
        "  )\n",
        "  config = AutoConfig.from_pretrained(\n",
        "      model_dir,\n",
        "      num_labels=cfg['num_labels'],\n",
        "      hidden_dropout_prob=hyperparams['hidden_dropout'],\n",
        "      attention_probs_dropout_prob=hyperparams['attention_probs_dropout'])\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=cfg['num_labels'])\n",
        "  if hyperparams['init']:\n",
        "    model = reinitialize_layers(model, hyperparams['init'])\n",
        "  model.config = AutoConfig.from_pretrained(model_dir, num_labels=cfg['num_labels'])\n",
        "  model.num_labels = cfg['num_labels']\n",
        "  if cfg['keep_layers']:\n",
        "    new_layers = torch.nn.ModuleList([layer_module for i, layer_module in enumerate(model.base_model.encoder.layer) if i in cfg['keep_layers']])\n",
        "    model.base_model.encoder.layer = new_layers\n",
        "    model.config.num_hidden_layers = len(cfg['keep_layers'])\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), correct_bias=hyperparams['bias'], lr=hyperparams['lr'])\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, num_training_steps=training_steps, num_warmup_steps=warmup_steps)\n",
        "  device = \"cuda:0\"\n",
        "  scores = []\n",
        "  best_score = 1.0\n",
        "  metric = load_metric(\"accuracy\")\n",
        "\n",
        "  class EvalCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, **kwargs):\n",
        "      if do_save_best:\n",
        "        model = kwargs['model']\n",
        "        y_pred = predict_fast(init_model=model, tokenizer=tokenizer, data=test_data, num_labels=cfg['num_labels'], is_multilabel=cfg['is_multilabel'])\n",
        "        model.train()\n",
        "        curr_score = rms(test_labels, y_pred) if not cfg['is_multilabel'] else metric.compute(predictions=y_pred, references=test_labels)['accuracy']\n",
        "        print('Score: ', curr_score)\n",
        "\n",
        "        if len(scores) == 0 or min(scores) > curr_score:\n",
        "          print(f'is min {curr_score} is smaller than {scores}')\n",
        "          best_score = curr_score\n",
        "          save_dir = os.path.join(out_dir, 'best')\n",
        "          model.save_pretrained(save_dir)\n",
        "          tokenizer.save_pretrained(save_dir)\n",
        "          with open(os.path.join(save_dir, 'hyperparams.txt'), 'w') as f:\n",
        "            hyperparams['score'] = curr_score\n",
        "            hyperparams['step'] = state.global_step\n",
        "            hyperparams['trainset_size'] = len(data_labels)\n",
        "            f.write(json.dumps(hyperparams))\n",
        "        scores.append(curr_score)\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "      args=training_args,                  # training arguments, defined above\n",
        "      train_dataset=train_dataset,         # training dataset\n",
        "      optimizers=(optimizer, scheduler),\n",
        "      callbacks=[EvalCallback]             # evaluation dataset\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  if not do_save_best:\n",
        "    model.save_pretrained(out_dir)\n",
        "    tokenizer.save_pretrained(out_dir)\n",
        "  print('Training done')\n",
        "\n",
        "  if do_save_best:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    return min(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flUcyrIbW7mE"
      },
      "source": [
        "def train_cv_v2(model_dir, out_dir, fold_dir, hyperparams, cfg, kfolds=[0, 1, 2, 3, 4, 5], continue_training=False, deduplicate=False, soft_label_model=None):\n",
        "  scores = []\n",
        "  for fold in kfolds:\n",
        "    train_df = pd.read_csv(fold_dir + '/train_fold_' + str(fold) + '.csv')\n",
        "    val_df = pd.read_csv(fold_dir + '/val_fold_' + str(fold) + '.csv')\n",
        "    if deduplicate:\n",
        "      train_df = train_df.drop_duplicates(subset=['excerpt'])\n",
        "    train_tx = [str(t) for t in train_df.excerpt.values]\n",
        "    train_sc = [float(t) for t in train_df.target.values]\n",
        "    val_tx = [str(t) for t in val_df.excerpt.values]\n",
        "    val_sc = [float(t) for t in val_df.target.values]\n",
        "\n",
        "    model_out_dir = out_dir + '/model_fold_' + str(fold)\n",
        "    if continue_training:\n",
        "      final_model_dir = model_dir + '/model_fold_' + str(fold) + '/best'\n",
        "    else:\n",
        "      final_model_dir = model_dir\n",
        "    \n",
        "    if cfg['soft_labels'] == 'add':\n",
        "      preds = predict_fast(final_model_dir, train_tx)\n",
        "      train_tx = train_tx + train_tx\n",
        "      train_sc = train_sc + preds\n",
        "    if cfg['soft_labels'] == 'only':\n",
        "      preds = predict_fast(final_model_dir, train_tx)\n",
        "      train_tx = train_tx\n",
        "      train_sc = preds\n",
        "    if soft_label_model and cfg['soft_labels'] == 'add':\n",
        "      preds = predict_fast(soft_label_model + '/model_fold_' + str(fold) + '/best', train_tx)\n",
        "      train_sc = train_sc + preds\n",
        "      train_tx = train_tx + train_tx\n",
        "    if soft_label_model and cfg['soft_labels'] == 'only':\n",
        "      preds = predict_fast(soft_label_model + '/model_fold_' + str(fold) + '/best', train_tx)\n",
        "      train_sc = preds\n",
        "      train_tx = train_tx\n",
        "      \n",
        "    best_score = train_model(\n",
        "        model_dir=final_model_dir,\n",
        "        out_dir=model_out_dir,\n",
        "        data=train_tx,\n",
        "        data_labels=train_sc,\n",
        "        test_data=val_tx,\n",
        "        test_labels=val_sc,\n",
        "        do_save_best=True,\n",
        "        hyperparams=hyperparams,\n",
        "        cfg=cfg\n",
        "      )\n",
        "    scores.append(best_score)\n",
        "  cv_score = np.mean(scores)\n",
        "  with open(out_dir + '/eval.txt', 'w') as f:\n",
        "    f.write('CV score is ' + str(cv_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnFw0t5lXY03"
      },
      "source": [
        "def predict_fast(model_name=None, data=None, init_model=None, tokenizer=None, num_labels=1, is_multilabel=False, output_logits=False, use_softmax=False):\n",
        "  device = \"cuda:0\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name) if model_name else tokenizer\n",
        "  config = AutoConfig.from_pretrained(model_name, num_labels=num_labels) if model_name else None\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config) if model_name else init_model\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  y_pred = []\n",
        "  batches = chunks(data, 32)\n",
        "  for batch in tqdm(batches):\n",
        "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention = inputs['attention_mask'].to(device)\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention\n",
        "    }\n",
        "    with torch.no_grad():        \n",
        "          outputs = model(**inputs)\n",
        "    if not use_softmax:\n",
        "      logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
        "    else:\n",
        "      logits = nn.functional.softmax(outputs.logits, dim=-1).detach().cpu().numpy().squeeze().tolist()\n",
        "    if is_multilabel and not output_logits:\n",
        "      logits = np.argmax(logits, axis=-1)\n",
        "    y_pred.extend(logits)\n",
        "  del model\n",
        "  gc.collect()\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9nwoUWhXfgM"
      },
      "source": [
        "def get_oof_predictions(model_dirs, fold_dir, out_dir, kfolds=[0,1,2,3,4,5]):\n",
        "  df = pd.DataFrame()\n",
        "  \n",
        "  for fold in kfolds:\n",
        "    val_df = pd.read_csv(fold_dir + '/val_fold_' + str(fold) + '.csv')\n",
        "    val_tx = [str(t) for t in val_df.excerpt.values]\n",
        "    val_sc = [float(t) for t in val_df.target.values]\n",
        "    fold_df = pd.DataFrame()\n",
        "    fold_df['fold'] = [fold for v in val_sc]\n",
        "    fold_df['excerpt'] = val_tx\n",
        "    fold_df['target'] = val_sc\n",
        "    fold_df['id'] = val_df['id']\n",
        "\n",
        "    for model in model_dirs:\n",
        "      final_model_dir = model + '/model_fold_' + str(fold) + '/best'\n",
        "      model_name = model.split('/')[-1]\n",
        "      preds = predict_fast(final_model_dir, val_tx)\n",
        "      fold_df[model_name] = preds\n",
        "    df = df.append(fold_df, ignore_index=True)\n",
        "  \n",
        "  df.to_csv(out_dir)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sehwzzrXqio"
      },
      "source": [
        "def train_leaky_ensembler(oof_dir, model_names, out_dir, kfolds=[0,1,2,3,4,5], model_bins=[], clf='ridge', find_opt_avg=False, bin_avg_dir=None, use_postprocessing=False):\n",
        "  df = pd.read_csv(oof_dir)\n",
        "\n",
        "  if find_opt_avg:\n",
        "    msk = np.random.rand(len(df)) < 0.2\n",
        "    df_test = df[msk].reset_index(drop=True)\n",
        "    df = df[~msk].reset_index(drop=True)\n",
        "    \n",
        "  get_bin_stratified(df, n_splits=6)\n",
        "\n",
        "  results = []\n",
        "  if find_opt_avg:\n",
        "    avg_df = pd.DataFrame()\n",
        "    avg_df['target'] = [float(f) for f in df_test['target']]\n",
        "  for fold in kfolds:\n",
        "    train_df = df.loc[df.fold!=fold].reset_index(drop=True)\n",
        "    val_df = df.loc[df.fold==fold].reset_index(drop=True)\n",
        "    \n",
        "    train_tx = [str(t) for t in train_df.excerpt.values]\n",
        "    val_tx = [str(t) for t in val_df.excerpt.values]\n",
        "    val_sc = [float(f) for f in val_df.target.values]\n",
        "    train_sc = [float(f) for f in train_df.target.values]\n",
        "\n",
        "    train_predictions = []\n",
        "    val_predictions = []\n",
        "    avg_predictions = []\n",
        "\n",
        "    if len(model_bins) > 0 and not use_postprocessing:\n",
        "      for model_name in model_bins:\n",
        "        preds = [json.loads(p) for p in train_df[model_name].values]\n",
        "        preds_val = [json.loads(p) for p in val_df[model_name].values]\n",
        "        if bin_avg_dir:\n",
        "          with open(bin_avg_dir, 'r') as f:\n",
        "            averages = json.loads(f.read())\n",
        "          preds = [averages[np.argmax(p)] for p in preds]\n",
        "          preds_val = [averages[np.argmax(p)] for p in preds_val]\n",
        "\n",
        "        train_predictions.append(preds)\n",
        "        val_predictions.append(preds_val)\n",
        "    \n",
        "    for model_name in model_names:\n",
        "      preds = [float(f) for f in train_df[model_name].values]\n",
        "      train_predictions.append(np.array(preds))\n",
        "      preds_val = [float(f) for f in val_df[model_name].values]\n",
        "      val_predictions.append(np.array(preds_val))\n",
        "      if find_opt_avg:\n",
        "        preds_avg = [float(f) for f in df_test[model_name].values]\n",
        "        avg_predictions.append(np.array(preds_avg))\n",
        "    \n",
        "    X = np.column_stack(train_predictions)\n",
        "    \n",
        "    if clf == 'ridge':\n",
        "      clf = Ridge(alpha=1.0)\n",
        "    elif clf == 'linearsvr':\n",
        "      clf = LinearSVR(max_iter=1000000)\n",
        "    elif clf == 'svr':\n",
        "      clf = SVR()\n",
        "    elif clf == 'kernel':\n",
        "      clf = KernelRidge()\n",
        "    elif clf == 'gbr':\n",
        "      clf = GradientBoostingRegressor()\n",
        "    elif clf == 'linear':\n",
        "      clf = LinearRegression()\n",
        "    elif clf == 'lasso':\n",
        "      clf = Lasso()\n",
        "    elif clf == 'bayes':\n",
        "      clf = BayesianRidge()\n",
        "    elif clf == 'perceptron':\n",
        "      clf = SGDRegressor()\n",
        "    \n",
        "    clf.fit(X, train_sc)\n",
        "\n",
        "    final_out = out_dir + '/model_fold_' + str(fold) + '/'\n",
        "    if not os.path.exists(os.path.dirname(final_out)):\n",
        "      try:\n",
        "          os.makedirs(os.path.dirname(final_out))\n",
        "      except OSError as exc: # Guard against race condition\n",
        "          if exc.errno != errno.EEXIST:\n",
        "              raise\n",
        "    dump(clf, final_out + 'ridge_model.joblib')\n",
        "\n",
        "    Y = np.column_stack(val_predictions)\n",
        "\n",
        "    y_preds = clf.predict(Y)\n",
        "    if use_postprocessing:\n",
        "      preds_val = [json.loads(p) for p in val_df[model_bins[0]].values]\n",
        "      with open(bin_avg_dir, 'r') as f:\n",
        "            averages = json.loads(f.read())\n",
        "      preds_val_bins = [np.argmax(p) for p in preds_val]\n",
        "      zipped = list(zip(preds_val_bins, preds_val))\n",
        "      y_preds = postprocess_predictions(y_preds, zipped, averages)\n",
        "\n",
        "    score = rms(val_sc, y_preds)\n",
        "    print('Score is: ', score)\n",
        "    results.append(score)\n",
        "\n",
        "    if find_opt_avg:\n",
        "      Y_test = np.column_stack(avg_predictions)\n",
        "      y_preds_test = clf.predict(Y_test)\n",
        "      avg_df['fold_' + str(fold)] = y_preds_test\n",
        "  \n",
        "  if find_opt_avg:\n",
        "    ridge_names = ['fold_' + str(fold) for fold in range(kfolds)]\n",
        "    print(find_best_stack(avg_df, ridge_names, drop_models=False))\n",
        "\n",
        "  with open(out_dir + '/eval.txt', 'w') as f:\n",
        "    mean = np.mean(results)\n",
        "    print('CV ist: ', mean)\n",
        "    f.write('CV is: ' + str(mean))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DBzdmgYEMy"
      },
      "source": [
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_GGOXV1bBJ1"
      },
      "source": [
        "def rms(y_actual, y_predicted):\n",
        "  return mean_squared_error(y_actual, y_predicted, squared=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjBEUthJYFYy"
      },
      "source": [
        "# Pretraining models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoQs5JkaYII6"
      },
      "source": [
        "# Load the pseudo-labeled training data for pretraining models\n",
        "train_df = pd.read_csv(os.path.join(BASE_PATH, 'data/training/predicted/predicted.csv'))\n",
        "train_tx = [str(t) for t in train_df.excerpt.values]\n",
        "train_sc = [float(t) for t in train_df.target.values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jxQx9i8ZI7X"
      },
      "source": [
        "# Load the entire training set from the original competition for validation during pretraining\n",
        "val_df = pd.read_csv(os.path.join(BASE_PATH, 'data/training/original/train.csv'))\n",
        "val_tx = [str(t) for t in train_df.excerpt.values]\n",
        "val_sc = [float(t) for t in train_df.target.values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIoSp7aLZR4-"
      },
      "source": [
        "# Train an ALBERT model\n",
        "\n",
        "model_name = 'albert-xxlarge-v2'\n",
        "hyperparams = {\n",
        "  'bs': 3,\n",
        "  'lr': 9e-6,\n",
        "  'weight_decay': 0.01,\n",
        "  'ep': 5,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.07,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 60,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "ALBERT_PRETRAINED = os.path.join(BASE_PATH, 'models/albertxxlarge2models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trfdyCkAaEgS"
      },
      "source": [
        "train_model(\n",
        "    model_dir=model_name,\n",
        "    out_dir=ALBERT_PRETRAINED,\n",
        "    data=train_tx,\n",
        "    data_labels=train_sc,\n",
        "    test_data=val_tx,\n",
        "    test_labels=val_sc,\n",
        "    do_save_best=True,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfjD9r7TayIc"
      },
      "source": [
        "# Train a DEBERTA model\n",
        "model_name = 'microsoft/deberta-large'\n",
        "hyperparams = {\n",
        "  'bs': 3,\n",
        "  'lr': 9e-6,\n",
        "  'weight_decay': 0.1,\n",
        "  'ep': 4,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 20,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "DEBERTA_PRETRAINED = os.path.join(BASE_PATH, 'models/debertabootstrap')\n",
        "\n",
        "train_model(\n",
        "    model_dir=model_name,\n",
        "    out_dir=out_dir,\n",
        "    data=train_tx,\n",
        "    data_labels=train_sc,\n",
        "    test_data=val_tx,\n",
        "    test_labels=val_sc,\n",
        "    do_save_best=True,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WSipbOPb2LW"
      },
      "source": [
        "# Train a RoBERTa model\n",
        "model_name = 'roberta-large'\n",
        "hyperparams = {\n",
        "  'bs': 8,\n",
        "  'lr': 1e-5,\n",
        "  'weight_decay': 0.01,\n",
        "  'ep': 4,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 10,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "ROBERTA_PRETRAINED = os.path.join(BASE_PATH, 'models/robertalargetwomodels')\n",
        "\n",
        "train_model(\n",
        "    model_dir=model_name,\n",
        "    out_dir=out_dir,\n",
        "    data=train_tx,\n",
        "    data_labels=train_sc,\n",
        "    test_data=val_tx,\n",
        "    test_labels=val_sc,\n",
        "    do_save_best=True,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKTR9SyZcasB"
      },
      "source": [
        "# Train an ELECTRA model\n",
        "model_name = 'google/electra-large-discriminator'\n",
        "hyperparams = {\n",
        "  'bs': 4,\n",
        "  'lr': 8e-6,\n",
        "  'weight_decay': 0.1,\n",
        "  'ep': 7,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 10,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "ELECTRA_PRETRAINED = os.path.join(BASE_PATH, 'models/electralarge')\n",
        "\n",
        "train_model(\n",
        "    model_dir=model_name,\n",
        "    out_dir=ELECTRA_PRETRAINED,\n",
        "    data=train_tx,\n",
        "    data_labels=train_sc,\n",
        "    test_data=val_tx,\n",
        "    test_labels=val_sc,\n",
        "    do_save_best=True,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZp4MeZUdFsm"
      },
      "source": [
        "# Training models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3deWnW2dLTv"
      },
      "source": [
        "In total, I trained 3 deberta-large, 1 roberta-large, 3 albert-xxlarge and 1 electra-large model for my winning submission.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ze54BKZdYvk"
      },
      "source": [
        "# Training the ALBERT models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRuHZIARd-GJ"
      },
      "source": [
        "# albert 1\n",
        "model_name = os.path.join(ALBERT_PRETRAINED, 'best')\n",
        "hyperparams = {\n",
        "  'bs': 3,\n",
        "  'lr': 9e-6,\n",
        "  'weight_decay': 0.01,\n",
        "  'ep': 5,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.07,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 10,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n",
        "out_dir = ALBERT_TRAINED_1\n",
        "\n",
        "train_cv_v2(\n",
        "    model_dir=model_name,\n",
        "    out_dir=out_dir,\n",
        "    fold_dir=fold_dir,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0YuArTWeglx"
      },
      "source": [
        "# albert 2\n",
        "model_name = os.path.join(ALBERT_PRETRAINED, 'best')\n",
        "hyperparams = {\n",
        "  'bs': 3,\n",
        "  'lr': 7e-6,\n",
        "  'weight_decay': 0.07,\n",
        "  'ep': 5,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 10,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n",
        "out_dir = ALBERT_TRAINED_2\n",
        "\n",
        "train_cv_v2(\n",
        "    model_dir=model_name,\n",
        "    out_dir=out_dir,\n",
        "    fold_dir=fold_dir,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2iQGSPFe7pW"
      },
      "source": [
        "# albert 3\n",
        "# albert 3 is special it is trained on all training data without evaluation.\n",
        "\n",
        "# YANISA ADD - so I can get this pretrained model I already have from Mathis but still train\n",
        "#   the new ALBERT_TRAINED_3\n",
        "ALBERT_PRETRAINED = os.path.join(BASE_PATH, 'models/albertxxlarge2models')\n",
        "\n",
        "model_name = os.path.join(ALBERT_PRETRAINED, 'best')\n",
        "hyperparams = {\n",
        "  'bs': 3,\n",
        "  'lr': 9e-6,\n",
        "  'weight_decay': 0.1,\n",
        "  'ep': 4,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 600,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(BASE_PATH, 'data/training/original/train.csv'))\n",
        "train_tx = [str(t) for t in train_df.excerpt.values]\n",
        "train_sc = [float(t) for t in train_df.target.values]\n",
        "\n",
        "out_dir = ALBERT_TRAINED_3\n",
        "\n",
        "\n",
        "train_model(\n",
        "   model_dir=model_name,\n",
        "   out_dir=out_dir,\n",
        "   data=train_tx,\n",
        "   data_labels=train_sc,\n",
        "   hyperparams=hyperparams,\n",
        "   cfg=cfg\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnKSaQWGpAUa"
      },
      "source": [
        "# Training the deberta models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXYFgmVApREy"
      },
      "source": [
        "# deberta 1\n",
        "model_name = os.path.join(DEBERTA_PRETRAINED, 'best')\n",
        "hyperparams = {\n",
        "  'bs': 3,\n",
        "  'lr': 9e-6,\n",
        "  'weight_decay': 0.1,\n",
        "  'ep': 4,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 10,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n",
        "out_dir = DEBERTA_TRAINED_1\n",
        "\n",
        "train_cv_v2(\n",
        "    model_dir=model_name,\n",
        "    out_dir=out_dir,\n",
        "    fold_dir=fold_dir,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPvQ-fImNmVl"
      },
      "source": [
        "# deberta 2\n",
        "model_name = os.path.join(DEBERTA_PRETRAINED, 'best')\n",
        "hyperparams = {\n",
        "  'bs': 3,\n",
        "  'lr': 7e-6,\n",
        "  'weight_decay': 0.1,\n",
        "  'ep': 4,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 10,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n",
        "out_dir = DEBERTA_TRAINED_2\n",
        "\n",
        "train_cv_v2(\n",
        "    model_dir=model_name,\n",
        "    out_dir=out_dir,\n",
        "    fold_dir=fold_dir,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEgot9YgN6at"
      },
      "source": [
        "# deberta 3\n",
        "# This deberta model was trained on data sampled using bootstrapping instead of cross validation\n",
        "# Only models trained on 2 folds/bags were used in the final submission\n",
        "model_name = os.path.join(DEBERTA_PRETRAINED, 'best')\n",
        "hyperparams = {\n",
        "  'bs': 3,\n",
        "  'lr': 9e-6,\n",
        "  'weight_decay': 0.08,\n",
        "  'ep': 4,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 10,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n",
        "out_dir = DEBERTA_TRAINED_3\n",
        "\n",
        "train_cv_v2(\n",
        "    model_dir=model_name,\n",
        "    out_dir=out_dir,\n",
        "    fold_dir=fold_dir,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg,\n",
        "    kfolds=[0,1]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKRTt-uUPVAm"
      },
      "source": [
        "# Training the ELECTRA model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiQ3rJsUPXSq"
      },
      "source": [
        "# electra 1\n",
        "model_name = os.path.join(ELECTRA_PRETRAINED, 'best')\n",
        "hyperparams = {\n",
        "  'bs': 3,\n",
        "  'lr': 8e-6,\n",
        "  'weight_decay': 0.1,\n",
        "  'ep': 5,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 10,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n",
        "out_dir = ELECTRA_TRAINED_1\n",
        "\n",
        "train_cv_v2(\n",
        "    model_dir=model_name,\n",
        "    out_dir=out_dir,\n",
        "    fold_dir=fold_dir,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcgBi10OP-_L"
      },
      "source": [
        "# Training the RoBERTa model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRWMXp9mQB2W"
      },
      "source": [
        "# roberta 1\n",
        "model_name = os.path.join(ROBERTA_PRETRAINED, 'best')\n",
        "hyperparams = {\n",
        "  'bs': 8,\n",
        "  'lr': 1e-5,\n",
        "  'weight_decay': 0.1,\n",
        "  'ep': 4,\n",
        "  'bias': True,\n",
        "  'init': None,\n",
        "  'hidden_dropout': 0.1,\n",
        "  'attention_probs_dropout': 0.1\n",
        "}\n",
        "cfg = {\n",
        "  'num_labels': 1,\n",
        "  'is_multilabel': False,\n",
        "  'logging_steps': 10,\n",
        "  'keep_layers': None,\n",
        "  'soft_labels': None\n",
        "}\n",
        "\n",
        "fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n",
        "out_dir = ROBERTA_TRAINED_1\n",
        "\n",
        "train_cv_v2(\n",
        "    model_dir=model_name,\n",
        "    out_dir=out_dir,\n",
        "    fold_dir=fold_dir,\n",
        "    hyperparams=hyperparams,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyZmZulYQ-5U"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHfTtt0QRC3B"
      },
      "source": [
        "model_dirs = [\n",
        "    ALBERT_TRAINED_1,\n",
        "    DEBERTA_TRAINED_1,\n",
        "    ALBERT_TRAINED_2,\n",
        "    DEBERTA_TRAINED_1,\n",
        "    ROBERTA_TRAINED_1,\n",
        "    ELECTRA_TRAINED_1\n",
        "]\n",
        "\n",
        "fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n",
        "out_dir = os.path.join(BASE_PATH, 'data/training/oof')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLtSkIirhe7O"
      },
      "source": [
        "get_oof_predictions(model_dirs=model_dirs, fold_dir=fold_dir, out_dir=out_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNFUoYZsh1GF"
      },
      "source": [
        "model_names_ensemble_1 = [\n",
        "    ALBERT_TRAINED_1.split('/')[-1],\n",
        "    DEBERTA_TRAINED_1.split('/')[-1],\n",
        "    ALBERT_TRAINED_2.split('/')[-1],\n",
        "    DEBERTA_TRAINED_1.split('/')[-1],\n",
        "    ROBERTA_TRAINED_1.split('/')[-1],\n",
        "    ELECTRA_TRAINED_1.split('/')[-1],      \n",
        "]\n",
        "\n",
        "model_names_ensemble_2 = model_names_ensemble_1[:-1]\n",
        "\n",
        "# oof_dir = os.path.join(BASE_PATH, 'data/training/oof')\n",
        "\n",
        "# out_dir_ensemble_1 = os.path.join(BASE_PATH, 'models/electraensembling')\n",
        "# out_dir_ensemble_2 = os.path.join(BASE_PATH, 'models/hugeensembler')\n",
        "# ^^ YANISA COMMENT - PUT BELOW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xB_cuoJuvaV"
      },
      "source": [
        "# YANISA ADD BLOCK FROM ABOVE ^^\n",
        "\n",
        "oof_dir = os.path.join(BASE_PATH, 'data/training/oof')\n",
        "\n",
        "out_dir_ensemble_1 = os.path.join(BASE_PATH, 'models/electraensembling')\n",
        "out_dir_ensemble_2 = os.path.join(BASE_PATH, 'models/hugeensembler')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ruz_111qWoQ"
      },
      "source": [
        "# train ensemble 1\n",
        "train_leaky_ensembler(oof_dir=oof_dir, model_names=model_names_ensemble_1, out_dir=out_dir_ensemble_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt_VtNCgqa0v"
      },
      "source": [
        "# train ensemble 2\n",
        "train_leaky_ensembler(oof_dir=oof_dir, model_names=model_names_ensemble_2, out_dir=out_dir_ensemble_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJLfireQ0uoS"
      },
      "source": [
        "You have finished training the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PQ6MVQywATv"
      },
      "source": [
        "**** YANISA ADD - I don't think I have to train anything here? Mathis included it in github"
      ]
    }
  ]
}