{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_clrp_external_data_labeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4PRHM_swWFm"
      },
      "source": [
        "# README\n",
        "\n",
        "This notebook is used to pseudo-label the additional data. Before running this notebook, you will need to run the external and competition data preparation notebooks.\n",
        "\n",
        "After this notebook, run the external relabeling notebook to label this data again.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtloB3Y39lTq"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BH988BowZCE"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install sentence-transformers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLeTpzWrxWUN"
      },
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch import nn\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeItphTc3x8F"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2rfLqXC60hR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR6vI0ykuc15"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB__EVF-uenf"
      },
      "source": [
        "\n",
        "BASE_PATH = 'gdrive/My Drive/colabNotebooks/commonLitReadabilityPrize/firstPlace_CodeFiles'\n",
        "PSEUDO_LABEL_MODEL_PATH = os.path.join(BASE_PATH, 'models/roberta-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRW0J5Y462Sb"
      },
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "SEED = 28\n",
        "seed_everything(seed=SEED)\n",
        "MAX_LENGTH = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxlIQpd1un1x"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQG27hNjrL5l"
      },
      "source": [
        "# YANISA - search_selection is the list of corpora, queries is the text from the training folds\n",
        "#   top_k is the n_samples from before (5 in this case)\n",
        "#   function inputs the data and corpora, takes the training sentences and finds the top 5 most similar sentences from the corpora\n",
        "#   and saves those 5 similar ones into 'selected' list of lists (returned)\n",
        "def search_similar_passages(queries, search_selection, top_k, model_name='paraphrase-TinyBERT-L6-v2', cuts=None):\n",
        "  model = SentenceTransformer(model_name)\n",
        "  bank = []\n",
        "  sentences = []\n",
        "  for dataset in search_selection:\n",
        "    in_dir = os.path.join(BASE_PATH, 'embeddings/encoded-' + dataset + '-' + model_name + '.pt')\n",
        "    if os.path.isfile(in_dir):\n",
        "      encoded = torch.load(in_dir)\n",
        "      bank.extend(encoded)\n",
        "    else:\n",
        "      raise FileNotFoundError(f'{dataset} embeddings could not be found.')\n",
        "    data_dir = os.path.join(BASE_PATH, 'data/preprocessed/' + dataset + '.csv')\n",
        "    if os.path.isfile(data_dir):\n",
        "      sents = pd.read_csv(data_dir)\n",
        "      sents = sents.text.values\n",
        "      sentences.extend(sents)\n",
        "    else:\n",
        "      raise FileNotFoundError(f'{dataset} passages could not be found.')\n",
        "    assert len(bank) == len(sentences)\n",
        "\n",
        "  print(f'Starting to search within {len(sentences)} text fragments...')\n",
        "  \n",
        "  encoded_queries = model.encode(queries, convert_to_tensor=True)\n",
        "\n",
        "  # YANISA - util.semantic_search doc https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.semantic_search\n",
        "  hits = util.semantic_search(encoded_queries, bank, top_k=top_k, corpus_chunk_size=80000)\n",
        "  selected = []\n",
        "  for hit in hits:\n",
        "    sents = [sentences[h['corpus_id']] for h in hit]\n",
        "\n",
        "    # if cuts:\n",
        "    #   sents = sents[cuts[0]:cuts[1]]\n",
        "    # ^^ original code\n",
        "    #   YANISA - commented out because they passed in a parameter of cuts=None\n",
        "\n",
        "    selected.append(sents)\n",
        "\n",
        "  return selected\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lagv6uH3vo62"
      },
      "source": [
        "# YANISA - idx = index\n",
        "#   inputs the 'hits' (list of lists from search_similar_passages function), scores, and standard error\n",
        "#   there is one score and standard error associated with each list inside the list ('hits' - the 5 most similar corpora sentences)\n",
        "#   so this function turns all of this into a list of tuples, where each tuple contains the (corpus sentence, score, standard error)\n",
        "def zip_hits_scores(hits, scores, stdev):\n",
        "  zipped = []\n",
        "  for idx, hit in enumerate(hits):\n",
        "    current = [(h, scores[idx], stdev[idx]) for h in hit]\n",
        "    zipped.extend(current)\n",
        "  return zipped\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RCtumYzvrzz"
      },
      "source": [
        "# YANISA - this one is filtering so we only keep the ones where the difference of the predictions minus the actual scores\n",
        "#   are less than the standard error\n",
        "def filter_on_stdev(sentences, predictions, scores, stdev):\n",
        "  pred_filtered = []\n",
        "  sents_filtered = []\n",
        "  for idx, pred in enumerate(predictions):\n",
        "    dev = stdev[idx]\n",
        "    gt = scores[idx]\n",
        "    diff = abs(pred - gt)\n",
        "    if diff < dev:\n",
        "      pred_filtered.append(pred)\n",
        "      sents_filtered.append(sentences[idx])\n",
        "  \n",
        "  return sents_filtered, pred_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgF-nPh6w5AD"
      },
      "source": [
        "def generate_augmented_data(fold_dir, model_dir, out_dir, n_samples=5, kfolds=[0, 1, 2, 3, 4, 5]):\n",
        "  for fold in kfolds:\n",
        "    torch.cuda.empty_cache()\n",
        "    train_fold = pd.read_csv(fold_dir + '/train_fold_' + str(fold) + '.csv')\n",
        "    val_fold = pd.read_csv(fold_dir + '/val_fold_' + str(fold) + '.csv')\n",
        "    queries = [str(t) for t in train_fold.excerpt.values]\n",
        "    scores = [float(t) for t in train_fold.target.values]\n",
        "    stdev = [float(t) for t in train_fold.standard_error.values]\n",
        "\n",
        "    #corpora = ['simplewiki', 'cb_corpus', 'wiki_snippets', 'onestop', 'asb', 'kaggle_scraped', 'bookcorpus_02']\n",
        "    #   ^^ original code - changed below because there was an issue w/ 'bookcorpus' filename - YANISA\n",
        "\n",
        "    corpora = ['simplewiki', 'cb_corpus', 'wiki_snippets', 'onestop', 'asb', 'kaggle_scraped', 'bookcorpus']\n",
        "    hits = search_similar_passages(queries, corpora, n_samples)\n",
        "    zipped = zip_hits_scores(hits, scores, stdev)\n",
        "    sentences = [t[0] for t in zipped]\n",
        "    scores = [t[1] for t in zipped]\n",
        "    stdev = [t[2] for t in zipped]\n",
        "    torch.cuda.empty_cache()\n",
        "    predictions = predict_fast(model_dir + '/model_fold_' + str(fold) + '/best', sentences)\n",
        "    print(len(predictions))\n",
        "\n",
        "    sents_filtered, preds_filtered = filter_on_stdev(sentences, predictions, scores, stdev)\n",
        "    augmented_df = pd.DataFrame.from_dict({'excerpt': sents_filtered, 'target': preds_filtered})\n",
        "    #augmented_df.to_csv(out_dir + '/predicted.csv')\n",
        "    # ^^ orig code - fixed below YANISA\n",
        "    augmented_df.to_csv(out_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wzHLOB9w5-X"
      },
      "source": [
        "def predict_fast(model_name=None, data=None, init_model=None, tokenizer=None, num_labels=1, is_multilabel=False, output_logits=False, use_softmax=False):\n",
        "  device = \"cuda:0\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name) if model_name else tokenizer\n",
        "  config = AutoConfig.from_pretrained(model_name, num_labels=num_labels) if model_name else None\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config) if model_name else init_model\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  y_pred = []\n",
        "  batches = chunks(data, 32)\n",
        "  for batch in tqdm(batches):\n",
        "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention = inputs['attention_mask'].to(device)\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention\n",
        "    }\n",
        "    with torch.no_grad():        \n",
        "          outputs = model(**inputs)\n",
        "    if not use_softmax:\n",
        "      logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
        "    else:\n",
        "      logits = nn.functional.softmax(outputs.logits, dim=-1).detach().cpu().numpy().squeeze().tolist()\n",
        "    if is_multilabel and not output_logits:\n",
        "      logits = np.argmax(logits, axis=-1)\n",
        "    y_pred.extend(logits)\n",
        "\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw_ED7B7x_F1"
      },
      "source": [
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFTyzUGqyJxw"
      },
      "source": [
        "# Labeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQYnRHqqyL48"
      },
      "source": [
        "fold_dir = os.path.join(BASE_PATH, 'data/training/cv')\n",
        "model_dir = os.path.join(BASE_PATH, 'models/roberta-base')\n",
        "out_dir = os.path.join(BASE_PATH, 'data/training/predicted/predicted.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM2V4ydT3EWY"
      },
      "source": [
        "generate_augmented_data(fold_dir=fold_dir, model_dir=model_dir, out_dir=out_dir, kfolds=[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}