{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fineTuneModel_SentenceBERT.ipynb","provenance":[{"file_id":"1X0dmtL4fiLNIDAz6R66gvXyZAgNb5HkZ","timestamp":1632881763750}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OWKnIrM9Q7ol"},"source":["This file fine-tunes the model which is later used for the sentence-bert model.\n","\n","Mostly written by Mathis Lucka ([GitHub](https://github.com/mathislucka), [Kaggle](https://www.kaggle.com/mathislucka))."]},{"cell_type":"markdown","metadata":{"id":"TnjnIIryQ4i0"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"D8hxdhjEmILI"},"source":["!pip install sentence-transformers\n","!pip install torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pz3hTTwTteLR"},"source":["from sentence_transformers import SentenceTransformer, util, losses, models\n","import torch\n","import numpy as np\n","import pandas as pd\n","import math\n","import random\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0p8i1995PbSi"},"source":["# Open up gdrive to get files\n","\n","from google.colab import drive\n","drive.mount('gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OK-rGM2pNuAi"},"source":["### GLOBAL VARIABLES ###\n","\n","BASE_PATH = 'gdrive/MyDrive/colabNotebooks/commonLitReadabilityPrize/firstPlace_CodeFiles'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gc3kOdVnAgO"},"source":["# Functions"]},{"cell_type":"code","metadata":{"id":"QCYo_gm3nZjp"},"source":["def normalize_scores(df):\n","  x = np.array(df.target.values)\n","  x -= x.min()\n","  x /= x.ptp()\n","  df['target_norm'] = x\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7QnlT33fnaae"},"source":["def draw_random_pairs(items, use_both_directions=False):\n","  to_prepare = items\n","\n","  split = math.floor(len(to_prepare)/2)\n","  random.shuffle(to_prepare)\n","  first_half = to_prepare[:split-1]\n","  second_half = to_prepare[split:-1]\n","\n","  results = []\n","  for item in first_half:\n","    selected = second_half.pop()\n","    results.append((item, selected))\n","    if use_both_directions:\n","      results.append((selected, item))\n","  \n","  return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Neb4sP_FnlsL"},"source":["def make_easy_hard_cv_data(fold_dir, out_dir, n_folds=6, use_both_directions=True):\n","  for fold in range(n_folds):\n","    train = pd.read_csv(fold_dir + '/train_fold_' + str(fold) + '.csv')\n","    val = pd.read_csv(fold_dir + '/val_fold_' + str(fold) + '.csv')\n","    \n","    train_norm = normalize_scores(train)\n","    val_norm = normalize_scores(val)\n","\n","    train_tx = [str(t) for t in train_norm['excerpt'].values]\n","    train_sc = [float(t) for t in train_norm['target_norm'].values]\n","    train_pairs = draw_random_pairs(list(zip(train_tx, train_sc)), use_both_directions=use_both_directions)\n","\n","    val_tx = [str(t) for t in val_norm['excerpt'].values]\n","    val_sc = [float(t) for t in val_norm['target_norm'].values]\n","    val_pairs = draw_random_pairs(list(zip(val_tx, val_sc)))\n","    train_easy_labels = []\n","    train_left = []\n","    train_right = []\n","    for pair in train_pairs:\n","      train_easy_labels.append(0 if pair[0][1] < pair[1][1] else 1)\n","      train_left.append(pair[0][0])\n","      train_right.append(pair[1][0])\n","\n","    train_df = pd.DataFrame.from_dict({\n","        'left_text': train_left,\n","        'right_text': train_right,\n","        'distance': train_easy_labels\n","    })\n","        \n","    train_df.to_csv(out_dir + '/train_fold_' + str(fold) + '_simplerAlgo' + '.csv')\n","\n","    # YANISA\n","    val_easy_labels = []\n","    val_left = []\n","    val_right = []\n","    for pair in val_pairs:\n","      val_easy_labels.append(0 if pair[0][1] < pair[1][1] else 1)\n","      val_left.append(pair[0][0])\n","      val_right.append(pair[1][0])\n","    \n","    val_df = pd.DataFrame.from_dict({\n","        'left_text': val_left,\n","        'right_text': val_right,\n","        'distance': val_easy_labels\n","    })\n","\n","    val_df.to_csv(out_dir + '/val_fold_' + str(fold) + '_simplerAlgo' + '.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pq2fGLinC3B"},"source":["def make_distance_cv_data(fold_dir, out_dir, n_folds=6, pseudolabel_dir=None, num_draws=1):\n","  for fold in range(n_folds):\n","    train = pd.read_csv(fold_dir + '/train_fold_' + str(fold) + '.csv')\n","    val = pd.read_csv(fold_dir + '/val_fold_' + str(fold) + '.csv')\n","    \n","    train_norm = normalize_scores(train)\n","    val_norm = normalize_scores(val)\n","\n","    train_tx = [str(t) for t in train_norm['excerpt'].values]\n","    train_sc = [float(t) for t in train_norm['target_norm'].values]\n","\n","    if pseudolabel_dir:\n","      pseudo = pd.read_csv(pseudolabel_dir)\n","      pseudo_norm = normalize_scores(pseudo)\n","      pseudo_tx = [str(t) for t in pseudo_norm['excerpt'].values]\n","      pseudo_sc = [float(t) for t in pseudo_norm['target_norm'].values]\n","      train_tx = train_tx + pseudo_tx\n","      train_sc = train_sc + pseudo_sc\n","\n","    train_pairs = []\n","    draws = 0\n","    while draws <= num_draws:\n","      train_pairs.extend(draw_random_pairs(list(zip(train_tx, train_sc))))\n","      draws += 1\n","\n","    val_tx = [str(t) for t in val_norm['excerpt'].values]\n","    val_sc = [float(t) for t in val_norm['target_norm'].values]\n","    val_pairs = draw_random_pairs(list(zip(val_tx, val_sc)))\n","\n","    train_distances = []\n","    train_left = []\n","    train_right = []\n","    for pair in train_pairs:\n","      distance = 1-abs(pair[0][1] - pair[1][1])\n","      train_left.append(pair[0][0])\n","      train_right.append(pair[1][0])\n","      train_distances.append(float(distance))\n","    \n","    train_df = pd.DataFrame.from_dict({\n","        'left_text': train_left,\n","        'right_text': train_right,\n","        'distance': train_distances\n","    })\n","\n","    train_df.to_csv(out_dir + '/train_fold_' + '_simplerAlgo' + str(fold) + '.csv')\n","\n","    val_distances = []\n","    val_left = []\n","    val_right = []\n","    for pair in val_pairs:\n","      distance = 1-abs(pair[0][1] - pair[1][1])\n","      val_left.append(pair[0][0])\n","      val_right.append(pair[1][0])\n","      val_distances.append(float(distance))\n","    \n","    val_df = pd.DataFrame.from_dict({\n","        'left_text': val_left,\n","        'right_text': val_right,\n","        'distance': val_distances\n","    })\n","\n","    val_df.to_csv(out_dir + '/val_fold_' + str(fold) + '_simplerAlgo' + '.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bEinM1c2uGld"},"source":["from torch.utils.data import DataLoader\n","import math\n","from sentence_transformers import LoggingHandler, util\n","from sentence_transformers.cross_encoder import CrossEncoder\n","from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator, CECorrelationEvaluator\n","from sentence_transformers import InputExample\n","import logging\n","from datetime import datetime\n","import sys\n","import os\n","import gzip\n","import csv\n","\n","def train_crossencoder(model_dir, out_dir, train_data, eval_data, hyperparams):\n","  \"\"\"\n","  Script taken from https://github.com/UKPLab/sentence-transformers\n","  This examples trains a CrossEncoder for the STSbenchmark task. A CrossEncoder takes a sentence pair\n","  as input and outputs a label. Here, it output a continious labels 0...1 to indicate the similarity between the input pair.\n","  It does NOT produce a sentence embedding and does NOT work for individual sentences.\n","  Usage:\n","  python training_stsbenchmark.py\n","  \"\"\"\n","\n","  #### Just some code to print debug information to stdout\n","  logging.basicConfig(format='%(asctime)s - %(message)s',\n","                      datefmt='%Y-%m-%d %H:%M:%S',\n","                      level=logging.INFO,\n","                      handlers=[LoggingHandler()])\n","  logger = logging.getLogger(__name__)\n","  #### /print debug information to stdout\n","\n","\n","\n","  #Define our Cross-Encoder\n","  train_batch_size = hyperparams['bs']\n","  num_epochs = hyperparams['ep']\n","  model_save_path = out_dir\n","\n","  model = CrossEncoder(model_dir, num_labels=hyperparams['num_labels'])\n","\n","  train_samples = []\n","  dev_samples = []\n","  for index, row in train_data.iterrows():\n","    train_samples.append(InputExample(texts=[row['left_text'], row['right_text']], label=row['distance']))\n","\n","  for index, row in eval_data.iterrows():\n","    dev_samples.append(InputExample(texts=[row['left_text'], row['right_text']], label=row['distance']))\n","\n","  train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n","\n","  # We add an evaluator, which evaluates the performance during training\n","  evaluator = CECorrelationEvaluator.from_input_examples(dev_samples, name='clrp-dev')\n","\n","\n","  # Configure the training\n","  warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n","  logger.info(\"Warmup-steps: {}\".format(warmup_steps))\n","\n","\n","  # Train the model\n","  model.fit(train_dataloader=train_dataloader,\n","            evaluator=evaluator,\n","            epochs=num_epochs,\n","            warmup_steps=warmup_steps,\n","            evaluation_steps=hyperparams['logging'],\n","            optimizer_params = {'lr': hyperparams['lr']},\n","            weight_decay=hyperparams['weight_decay'],\n","            output_path=model_save_path,\n","            save_best_model=True,\n","            use_amp=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9lSu8lmmoH3v"},"source":["def train_sent_transformer_cv(fold_dir, model_dir, out_dir, hyperparams, kfolds=[0,1,2,3,4,5], continue_training=False, use_crossencoder=False):\n","  for fold in kfolds:\n","    torch.cuda.empty_cache()\n","    train_data = pd.read_csv(fold_dir + '/train_fold_' + str(fold) + '_simplerAlgo' + '.csv')\n","    val_data = pd.read_csv(fold_dir + '/val_fold_' + str(fold) + '_simplerAlgo' + '.csv')\n","    if continue_training:\n","      final_model_dir = model_dir + '/model_fold_' + str(fold) + '_simplerAlgo'\n","    else:\n","      final_model_dir = model_dir\n","    \n","    if use_crossencoder:\n","      train_crossencoder(\n","          model_dir=final_model_dir,\n","          out_dir=out_dir + '/model_fold_' + str(fold) + '_simplerAlgo',\n","          train_data=train_data,\n","          eval_data=val_data,\n","          hyperparams=hyperparams\n","      )\n","    else:\n","      train_bi_encoder(\n","          model_dir=final_model_dir,\n","          out_dir=out_dir + '/model_fold_' + str(fold) + '_simplerAlgo',\n","          train_data=train_data,\n","          eval_data=val_data,\n","          hyperparams=hyperparams\n","      )   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TiTGwItloIvD"},"source":["from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n","from sentence_transformers.evaluation import TripletEvaluator\n","\n","def train_bi_encoder(model_dir, out_dir, train_data, eval_data, hyperparams):\n","  print('called sent trans', train_data.head())\n","  #### Just some code to print debug information to stdout\n","  logging.basicConfig(format='%(asctime)s - %(message)s',\n","                      datefmt='%Y-%m-%d %H:%M:%S',\n","                      level=logging.INFO,\n","                      handlers=[LoggingHandler()])\n","\n","  train_batch_size = hyperparams['bs']\n","  model_save_path = out_dir\n","  max_seq_length = 256\n","\n","\n","  # Use Huggingface/transformers model (like BERT, RoBERTa, XLNet, XLM-R) for mapping tokens to embeddings\n","  word_embedding_model = models.Transformer(model_dir, max_seq_length=max_seq_length)\n","\n","  # Apply mean pooling to get one fixed sized sentence vector\n","  pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n","                                pooling_mode_mean_tokens=True,\n","                                pooling_mode_cls_token=False,\n","                                pooling_mode_max_tokens=False)\n","\n","  model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","  \n","  train_samples = []\n","  dev_samples = []\n","  for index, row in train_data.iterrows():\n","    if hyperparams['task_type'] == 'sts':\n","      train_samples.append(InputExample(texts=[row['left_text'], row['right_text']], label=row['distance']))\n","    elif hyperparams['task_type'] == 'triplet':\n","      train_samples.append(InputExample(texts=[row['anchor'], row['similar'], row['dissimilar']], label=0))\n","  \n","  for index, row in eval_data.iterrows():\n","    if hyperparams['val_type'] == 'sts':\n","      dev_samples.append(InputExample(texts=[row['left_text'], row['right_text']], label=row['distance']))\n","    elif hyperparams['val_type'] == 'triplet':\n","      dev_samples.append(InputExample(texts=[row['anchor'], row['similar'], row['dissimilar']], label=0))\n","  \n","  train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n","  if hyperparams['task_type'] == 'sts':\n","    train_loss = losses.CosineSimilarityLoss(model=model)\n","  elif hyperparams['task_type'] == 'triplet':\n","    train_loss = losses.TripletLoss(model=model)\n","  \n","  if hyperparams['val_type'] == 'sts':\n","    dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, batch_size=train_batch_size, name='clrp-dev')\n","  elif hyperparams['val_type'] == 'triplet':\n","    dev_evaluator = TripletEvaluator.from_input_examples(dev_samples, name='clrp-dev')\n","\n","\n","  warmup_steps = math.ceil(len(train_dataloader) * hyperparams['ep'] * 0.1) #10% of train data for warm-up\n","  logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n","\n","  # Train the model\n","  model.fit(train_objectives=[(train_dataloader, train_loss)],\n","            evaluator=dev_evaluator,\n","            epochs=hyperparams['ep'],\n","            evaluation_steps=hyperparams['logging'],\n","            warmup_steps=warmup_steps,\n","            output_path=model_save_path,\n","            optimizer_params = {'lr': hyperparams['lr']},\n","            weight_decay=hyperparams['weight_decay']\n","            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcWlMJcyoo-T"},"source":["def prepare_triplets(df, n_samples, replace=False, margin=0.3):\n","  triplets= []\n","  while len(triplets) < n_samples:\n","    samples = df.sample(n=3, replace=replace)\n","    sample_texts = [str(t) for t in samples.excerpt.values]\n","    sample_scores = [float(s) for s in samples.target_norm.values]\n","    anchor_idx = random.randrange(0, 3)\n","    anchor_text = sample_texts.pop(anchor_idx)\n","    anchor_score = sample_scores.pop(anchor_idx)\n","    triplet = [anchor_text]\n","    sim_1 = 1-(abs(anchor_score - sample_scores[0]))\n","    sim_2 = 1-(abs(anchor_score - sample_scores[1]))\n","    if abs(sim_1 - sim_2) < margin:\n","      continue\n","    text_1 = sample_texts[0]\n","    text_2 = sample_texts[1]\n","    if sim_1 >= sim_2:\n","      triplet.extend([text_1, text_2, sim_1, sim_2])\n","    else:\n","      triplet.extend([text_2, text_1, sim_2, sim_1])\n","    triplets.append(triplet)\n","  \n","  return triplets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKzw5CT7opuI"},"source":["def make_cv_triplet_data(fold_dir, out_dir, n_samples, kfolds=[0,1,2,3,4,5], replace=True, margin=0.3, remove_duplicates=True, generate_dev=True):\n","  for fold in kfolds:\n","    train_df = normalize_scores(pd.read_csv(fold_dir + '/train_fold_' + str(fold) + '.csv'))\n","    val_df = normalize_scores(pd.read_csv(fold_dir + '/val_fold_' + str(fold) + '.csv'))\n","    train_triplets = prepare_triplets(train_df, n_samples, replace, margin)\n","    if remove_duplicates:\n","      train_triplets = list(set(tuple(sub) for sub in train_triplets))\n","    if generate_dev:\n","      val_triplets = prepare_triplets(val_df, math.floor(n_samples * 0.2), replace, margin)\n","      if remove_duplicates:\n","        val_triplets = list(set(tuple(sub) for sub in val_triplets))\n","    train_triplet_df = pd.DataFrame(train_triplets, columns=['anchor', 'similar', 'dissimilar', 'sim_similar', 'sim_dissimilar'])\n","    train_triplet_df.to_csv(out_dir + '/train_fold_' + str(fold) + '_simplerAlgo' + '.csv')\n","    if generate_dev:\n","      val_triplet_df = pd.DataFrame(val_triplets, columns=['anchor', 'similar', 'dissimilar', 'sim_similar', 'sim_dissimilar'])\n","      val_triplet_df.to_csv(out_dir + '/val_fold_' + str(fold) + '_simplerAlgo' + '.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"auTSgxfFo4Ej"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"23TVu2WKo6Hj"},"source":["I used a few different strategies for training bi-encoders. Results were similar but they might change if more data is available. You can prepare the data in a few different ways and use different loss-functions for training. There is also the possibility to train a model on some intermediate task first. This gives slightly better results in my experience."]},{"cell_type":"code","metadata":{"id":"CR_bfu0ipYnh"},"source":["# First, we prepare the data for training.\n","fold_dir = os.path.join(BASE_PATH, 'data/training/simplerAlgo/cv') # structure is the same as in other notebooks (train_fold_0.csv, ...)\n","pseudo_labeled_dir = os.path.join(BASE_PATH, 'data/training/simplerAlgo/predicted/predicted.csv') # the dataframe that was pseudo-labeled\n","distance_data_dir = os.path.join(BASE_PATH, 'data/training/simplerAlgo/trainingData1')\n","easy_hard_data_dir = os.path.join(BASE_PATH, 'data/training/simplerAlgo/trainingData2')\n","triplet_data_dir = os.path.join(BASE_PATH, 'data/training/simplerAlgo/trainingData3')\n","\n","# this will generate random pairs of excerpts with a distance score between 0 and 1\n","make_distance_cv_data(fold_dir=fold_dir, out_dir=distance_data_dir, pseudolabel_dir=pseudo_labeled_dir, num_draws=1)\n","\n","# this will generate random pairs of excerpts were one excerpt is labeled 0 if it is easier than the other, or 1 if it is harder\n","make_easy_hard_cv_data(fold_dir, easy_hard_data_dir)\n","\n","# this will generate random triplets of excerpts following the schema of: anchor, positive (closer), negative (farther)\n","make_cv_triplet_data(fold_dir, triplet_data_dir, 10000, margin=0.2, generate_dev=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Q3C-Y4qr81U"},"source":["# We will now train a cross-encoder on the easy/hard data as an intermediate training step\n","\n","#### IF CHANGING THE MODEL BELOW, ALSO HAVE TO CHANGE IT IN THE NEXT BLOCK FINAL MODEL ####\n","base_model = 'roberta-base'\n","base_model_save_dir = os.path.join(BASE_PATH, 'models/roberta-base_simplerAlgo')\n","\n","hyperparams = {\n","    'ep': 5,\n","    'bs': 8, # changed batch size (Mathis had 16)\n","    'logging': 500,\n","    'lr': 2e-5,\n","    'weight_decay': 0.01,\n","    'num_labels': 1,\n","    'task_type': 'sts',\n","    'val_type': 'sts'\n","}\n","\n","train_sent_transformer_cv(fold_dir=easy_hard_data_dir, out_dir=base_model_save_dir, model_dir=base_model, hyperparams=hyperparams, use_crossencoder=True, kfolds=[0,1,2,3,4,5], continue_training=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQKpoMvKs3hS"},"source":["# Afterwards, we will use the distance data to train a bi-encoder\n","final_model_save_dir = os.path.join(BASE_PATH, 'models/finalModel_robertabase_simplerAlgo')\n","\n","hyperparams = {\n","    'ep': 5,\n","    'bs': 8, \n","    'logging': 500,\n","    'lr': 2e-5,\n","    'weight_decay': 0.01,\n","    'num_labels': 1,\n","    'task_type': 'sts',\n","    'val_type': 'sts'\n","}\n","\n","train_sent_transformer_cv(fold_dir=distance_data_dir, out_dir=final_model_save_dir, model_dir=base_model_save_dir, hyperparams=hyperparams, use_crossencoder=False, kfolds=[0,1,2,3,4,5], continue_training=True)"],"execution_count":null,"outputs":[]}]}