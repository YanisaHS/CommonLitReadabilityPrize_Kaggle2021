{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_clrp_inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVxUi7YlWDq6"
      },
      "source": [
        "# README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwTVLCTXWGuR"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6JW9m3nWIf-"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install sklearn\n",
        "!pip install datasets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgAHilC2WZ16"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import itertools\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "import gzip\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForMaskedLM, DataCollatorForWholeWordMask, DataCollatorForLanguageModeling, pipeline\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, TrainerCallback\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import shutil\n",
        "from datasets import load_metric\n",
        "import gc\n",
        "gc.enable()\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import Lasso, BayesianRidge, Perceptron, SGDRegressor\n",
        "\n",
        "# YANISA ADD\n",
        "#from pickle import load\n",
        "from joblib import load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqQyqJrWYblS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtQiZchuYObs"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPuHJc1eYQJE"
      },
      "source": [
        "BASE_PATH = 'gdrive/My Drive/colabNotebooks/commonLitReadabilityPrize/firstPlace_CodeFiles'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIOcjB6-bHPj"
      },
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "SEED = 28\n",
        "seed_everything(seed=SEED)\n",
        "MAX_LENGTH = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEj-F1o6xHZC"
      },
      "source": [
        "\n",
        "# fine-tuned model paths\n",
        "# adjust path if you have saved the models in different directories\n",
        "ALBERT_TRAINED_1 = os.path.join(BASE_PATH, 'models/albertxxlarge2models')\n",
        "ALBERT_TRAINED_2 = os.path.join(BASE_PATH, 'models/albertxxlargelowlr')\n",
        "ALBERT_TRAINED_3 = os.path.join(BASE_PATH, 'models/albertxxlargealldata')\n",
        "DEBERTA_TRAINED_1 = os.path.join(BASE_PATH, 'models/debertalarge')\n",
        "DEBERTA_TRAINED_2 = os.path.join(BASE_PATH, 'models/debertalargelowlr')\n",
        "DEBERTA_TRAINED_3 = os.path.join(BASE_PATH, 'models/debertabootstrap')\n",
        "ROBERTA_TRAINED_1 = os.path.join(BASE_PATH, 'models/robertalargetwomodels')\n",
        "ELECTRA_TRAINED_1 = os.path.join(BASE_PATH, 'models/electralarge')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDuIc01S1kJC"
      },
      "source": [
        "# ensemble model paths\n",
        "\n",
        "#RIDGE_ENSEMBLE_1 = os.path.join(BASE_PATH, 'models/electraensembling')\n",
        "# TEMP COMMENTING OUT ^^^ re-downloaded the ensemble below\n",
        "RIDGE_ENSEMBLE_1 = os.path.join(BASE_PATH, 'models/electra_larger_ensemble')\n",
        "\n",
        "RIDGE_ENSEMBLE_2 = os.path.join(BASE_PATH, 'models/hugeensemble')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkLqmtm8Wsck"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnFw0t5lXY03"
      },
      "source": [
        "def predict_fast(model_name=None, data=None, init_model=None, tokenizer=None, num_labels=1, is_multilabel=False, output_logits=False, use_softmax=False):\n",
        "  device = \"cuda:0\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name) if model_name else tokenizer\n",
        "  config = AutoConfig.from_pretrained(model_name, num_labels=num_labels) if model_name else None\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config) if model_name else init_model\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  y_pred = []\n",
        "  batches = chunks(data, 32)\n",
        "  for batch in tqdm(batches):\n",
        "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention = inputs['attention_mask'].to(device)\n",
        "    inputs = {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention\n",
        "    }\n",
        "    with torch.no_grad():        \n",
        "          outputs = model(**inputs)\n",
        "    if not use_softmax:\n",
        "      logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
        "    else:\n",
        "      logits = nn.functional.softmax(outputs.logits, dim=-1).detach().cpu().numpy().squeeze().tolist()\n",
        "    if is_multilabel and not output_logits:\n",
        "      logits = np.argmax(logits, axis=-1)\n",
        "    y_pred.extend(logits)\n",
        "  del model\n",
        "  gc.collect()\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DBzdmgYEMy"
      },
      "source": [
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_GGOXV1bBJ1"
      },
      "source": [
        "def rms(y_actual, y_predicted):\n",
        "  return mean_squared_error(y_actual, y_predicted, squared=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1MlM14B2pIw"
      },
      "source": [
        "def make_ensembler_predictions(fold_predictions, ensembler_dirs, return_mean=True):\n",
        "  final_predictions = []\n",
        "  for idx, predictions in enumerate(fold_predictions):\n",
        "\n",
        "    #clf = load(ensembler_dirs[idx])\n",
        "    # orig code ^^ edit YANISA\n",
        "\n",
        "    # YANISA EDITED - can't open file how Mathis had it\n",
        "    clf = load(open(ensembler_dirs[idx]))#, 'rb')) # 'rb' is to read as a binary file not as .txt file\n",
        "\n",
        "    Y = np.column_stack(predictions)\n",
        "    y_preds = clf.predict(Y)\n",
        "    final_predictions.append(y_preds)\n",
        "  \n",
        "  if return_mean:\n",
        "    preds = np.vstack(final_predictions)\n",
        "    del final_predictions\n",
        "    return np.mean(preds, axis=0)\n",
        "  else:\n",
        "    return final_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjBEUthJYFYy"
      },
      "source": [
        "# Load test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoQs5JkaYII6"
      },
      "source": [
        "# You will need to place the test data in /data/test/test.csv\n",
        "test_df = pd.read_csv(os.path.join(BASE_PATH, 'data/test/test.csv'))\n",
        "test_tx = [str(t) for t in test_df.excerpt.values]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1BmLqC3JTf"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jxQx9i8ZI7X"
      },
      "source": [
        "# Getting transformer predictions\n",
        "\n",
        "model_dirs = [\n",
        "    ALBERT_TRAINED_1,\n",
        "    DEBERTA_TRAINED_1,\n",
        "    ALBERT_TRAINED_2,\n",
        "    DEBERTA_TRAINED_2,\n",
        "    ROBERTA_TRAINED_1,\n",
        "    ELECTRA_TRAINED_1\n",
        "]\n",
        "\n",
        "fold_predictions = {\n",
        "    'fold_0': [],\n",
        "    'fold_1': [],\n",
        "    'fold_2': [],\n",
        "    'fold_3': [],\n",
        "    'fold_4': [],\n",
        "    'fold_5': [],\n",
        "}\n",
        "\n",
        "for i in range(6):\n",
        "  for model in model_dirs:\n",
        "    preds = predict_fast(model_name=os.path.join(model, 'model_fold_' + str(i) + '/best'), data=test_tx)\n",
        "    fold_predictions['fold_' + str(i)].append(np.array(preds))\n",
        "\n",
        "# Getting predictions from special models\n",
        "albert_single_preds = predict_fast(model_name=os.path.join(ALBERT_TRAINED_3, 'best'), data=test_tx)\n",
        "deberta_bs_0 = predict_fast(model_name=os.path.join(DEBERTA_TRAINED_3, 'model_fold_0/best'), data=test_tx)\n",
        "deberta_bs_1 = predict_fast(model_name=os.path.join(DEBERTA_TRAINED_3, 'model_fold_1/best'), data=test_tx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqyyDlJ95gXp"
      },
      "source": [
        "# Ensembling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIoSp7aLZR4-"
      },
      "source": [
        "ridge_dirs_1 = []\n",
        "ridge_dirs_2 = []\n",
        "\n",
        "for i in [1,2,4,5]:\n",
        "  #ridge_dirs_1.append(os.path.join(RIDGE_ENSEMBLE_1, 'model_fold_' + str(i) + 'ridge_model.joblib'))\n",
        "  # orig code ^^ fixed filepath\n",
        "\n",
        "  ridge_dirs_1.append(os.path.join(RIDGE_ENSEMBLE_1, 'model_fold_' + str(i), 'ridge_model.joblib'))\n",
        "\n",
        "for i in range(6):\n",
        "  #ridge_dirs_2.append(os.path.join(RIDGE_ENSEMBLE_2, 'model_fold_' + str(i) + 'ridge_model.joblib'))\n",
        "  #  orig code ^^ fixed filepath\n",
        "\n",
        "  ridge_dirs_2.append(os.path.join(RIDGE_ENSEMBLE_2, 'model_fold_' + str(i), 'ridge_model.joblib'))\n",
        "\n",
        "\n",
        "ensemble_1_preds = make_ensembler_predictions(\n",
        "    fold_predictions=[fold_predictions['fold_' + str(i)] for i in [1,2,4,5]],\n",
        "    ensembler_dirs=ridge_dirs_1\n",
        ")\n",
        "\n",
        "ensemble_2_preds = make_ensembler_predictions(\n",
        "    fold_predictions=[fold_predictions['fold_' + str(i)] for i in range(6)],\n",
        "    ensembler_dirs=ridge_dirs_2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVmyeHfa7nVQ"
      },
      "source": [
        "bs_mean_preds = np.array(deberta_bs_0) * 0.5 + np.array(deberta_bs_1) * 0.5\n",
        "bs_alb_mean_preds = np.array(albert_single_preds) * 0.65 + np.array(bs_mean_preds) * 0.35"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeHbkl7P8IEn"
      },
      "source": [
        "final_predictions = np.array(ensemble_1_preds) * 3./8. + np.array(ensemble_2_preds) * 2./8. + np.array(bs_alb_mean_preds) * 3./8."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RzkUikD8gVD"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZtBGVv48TSq"
      },
      "source": [
        "submission_df = pd.DataFrame({'id': test_df.id, 'target': final_predictions})\n",
        "submission_df.to_csv(os.path.join(BASE_PATH, 'data/submission/submission.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}